---
title: "3. Structured State Space Models (S4) Applied to RL as a replacement of Transformers"
excerpt: "Implemented the innovative S4 model as a
replacement for a Transformer. Reduced by 86% the number of trainable parameters required."
collection: portfolio
---

### Sequential Decision Modelling using Structured State Spaces

## Overview

This project explores the application of sequence models to the problem of credit assignment in Reinforcement Learning (RL). Specifically, it involves implementing a novel architecture, the Structured State Space Sequence Model (S4), within the framework of Decision Transformers. The aim is to improve the agent's ability to determine the true source of returns from a trajectory of states and actions, especially in environments with long episodes and sparse rewards.

### Key Objectives

1. **S4 Implementation**: Replace the transformer-based sequence model in the Decision Transformer framework with the S4 architecture to handle longer trajectories more effectively.
2. **Performance Benchmarking**: Compare the performance of the S4 Decision Model (S4DM) against the baseline Decision Transformer and Behavioral Cloning models across various RL environments.
3. **Hyperparameter Tuning**: Conduct a grid search to identify optimal hyperparameters for the S4 model to maximize performance.

### Methodology

- **Data Collection**: The project uses datasets from the OpenAI MuJoCo platform, including environments such as Half-Cheetah, Walker2D, and Hopper. These datasets contain trajectories generated by sub-optimal agents.
- **Sequence Modelling**: The S4 architecture, which leverages the HiPPO kernel for efficient sequence modelling, is integrated into the Decision Transformer framework.
- **Experimental Setup**: Experiments are conducted on three MuJoCo environments with datasets of "medium" and "medium-replay" trajectories. Each experiment is run multiple times with different seeds to ensure accuracy and reliability of results.

### Findings

- **Performance Improvement**: The S4DM model consistently outperformed the baseline Decision Transformer and Behavioral Cloning models across all environments and datasets in terms of maximum reward obtained.
- **Hyperparameter Sensitivity**: The performance of the S4DM model was sensitive to hyperparameter settings, with the best results achieved using a learning rate of 6.1e-4, a kernel size of 256 dimensions, and 16 S4 layers.

### Conclusion

The project demonstrates that the S4 architecture can effectively enhance the performance of Decision Transformers in RL tasks, particularly in environments with long trajectories. The results suggest that the increased "memory" of the S4 model, enabled by the HiPPO kernel, provides a distinct advantage in assigning credit to critical actions in a trajectory.

### Future Work

Future research will focus on testing the S4DM model in a wider range of environments, particularly those with very long trajectories, and further tuning the hyperparameters to fully realize the potential of the S4 architecture.

### Repository and Resources

To read the full analysis, you can download the [PDF file](https://vitoriarlima.github.io/files/S4_decision_transformer.pdf).

Slides deck presentation of the results [here](https://docs.google.com/presentation/d/1zFm0WUj-mHd8MR1ytRNR7XZ_lBN9gw8SU58nzEDB0L8/edit?usp=sharing).

---

### Pretty Visualizations

![HalfCheetah](/files/s4dm.gif)

![Walker2d](/files/s4dm1.gif)
